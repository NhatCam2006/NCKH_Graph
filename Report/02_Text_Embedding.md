# Part 2: TEXT EMBEDDING - CHUY·ªÇN TEXT TH√ÄNH VECTORS

**File code t∆∞∆°ng ·ª©ng:** `text_embedding.py`  
**Input:** `processed/jobs_processed.csv` (combined_text column)  
**Output:** `processed/job_embeddings.npy`, `processed/similarity_matrix.npy`

---

## üìö M·ª§C L·ª§C

1. [Embedding l√† g√¨?](#1-embedding-l√†-g√¨)
2. [T·∫°i sao c·∫ßn Embedding?](#2-t·∫°i-sao-c·∫ßn-embedding)
3. [Sentence Transformers](#3-sentence-transformers)
4. [T√≠nh Similarity Matrix](#4-t√≠nh-similarity-matrix)
5. [Code chi ti·∫øt](#5-code-chi-ti·∫øt)
6. [K·∫øt qu·∫£](#6-k·∫øt-qu·∫£)
7. [FAQ](#7-faq)

---

## 1. EMBEDDING L√Ä G√å?

### ƒê·ªãnh nghƒ©a ƒë∆°n gi·∫£n:

**Embedding** = Chuy·ªÉn text (ho·∫∑c b·∫•t k·ª≥ data n√†o) th√†nh **vector s·ªë**

```
Text                    ‚Üí    Vector (embedding)
"K·∫ø To√°n Thu·∫ø"          ‚Üí    [0.2, -0.5, 0.8, ..., 0.1]  (384 chi·ªÅu)
```

### T·∫°i sao l√† vector?

Computer **kh√¥ng hi·ªÉu text**, ch·ªâ hi·ªÉu **s·ªë**!

```
‚ùå Computer kh√¥ng th·ªÉ t√≠nh to√°n v·ªõi text:
   "K·∫ø To√°n" + "Nh√¢n Vi√™n" = ???

‚úì Computer c√≥ th·ªÉ t√≠nh to√°n v·ªõi vectors:
   [0.2, 0.5] + [0.3, 0.1] = [0.5, 0.6]
```

### V√≠ d·ª• ƒë∆°n gi·∫£n:

**One-Hot Encoding** (c√°ch c∆° b·∫£n nh·∫•t):

```python
Vocabulary: ["k·∫ø", "to√°n", "nh√¢n", "vi√™n"]

"k·∫ø to√°n"  ‚Üí [1, 1, 0, 0]
"nh√¢n vi√™n" ‚Üí [0, 0, 1, 1]
```

**V·∫•n ƒë·ªÅ:** Vector qu√° d√†i, kh√¥ng capture meaning!

**Sentence Embedding** (c√°ch hi·ªán ƒë·∫°i):

```python
"k·∫ø to√°n"  ‚Üí [0.2, -0.5, 0.8, 0.3, ...]  (384 chi·ªÅu)
"nh√¢n vi√™n" ‚Üí [0.1, -0.4, 0.6, 0.2, ...]  (384 chi·ªÅu)
```

**∆Øu ƒëi·ªÉm:** 
- Vector ng·∫Øn h∆°n (384 vs 10,000s)
- Capture **semantic meaning** (nghƒ©a)
- Similar words ‚Üí similar vectors

---

## 2. T·∫†I SAO C·∫¶N EMBEDDING?

### M·ª•c ti√™u: T√¨m jobs t∆∞∆°ng t·ª±

**C√¢u h·ªèi:** L√†m sao bi·∫øt 2 jobs "t∆∞∆°ng t·ª±" nhau?

#### ‚ùå C√°ch 1: So s√°nh text tr·ª±c ti·∫øp
```python
job1 = "K·∫ø To√°n Thu·∫ø"
job2 = "Accountant"

if job1 == job2:  # False!
    print("Similar")
```

**V·∫•n ƒë·ªÅ:** 
- Kh√°c ng√¥n ng·ªØ ‚Üí kh√¥ng match
- Synonym (t·ª´ ƒë·ªìng nghƒ©a) ‚Üí kh√¥ng match
- Ch·ªâ match **exact text**

#### ‚úì C√°ch 2: So s√°nh embeddings
```python
job1_vec = [0.2, -0.5, 0.8, ...]  # "K·∫ø To√°n Thu·∫ø"
job2_vec = [0.3, -0.4, 0.7, ...]  # "Accountant"

similarity = cosine_similarity(job1_vec, job2_vec)
# ‚Üí 0.95 (very similar!)
```

**∆Øu ƒëi·ªÉm:**
- Hi·ªÉu **nghƒ©a** (semantic)
- Cross-lingual (nhi·ªÅu ng√¥n ng·ªØ)
- T√¨m ƒë∆∞·ª£c similar jobs ngay c·∫£ khi text kh√°c nhau

---

## 3. SENTENCE TRANSFORMERS

### Model ƒë∆∞·ª£c d√πng:

```python
model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
```

### ƒê·∫∑c ƒëi·ªÉm:

| Feature | Value |
|---------|-------|
| **Model type** | Sentence Transformer |
| **Base model** | MiniLM (Microsoft) |
| **Languages** | Multilingual (50+ languages) |
| **Vietnamese support** | ‚úÖ YES |
| **Embedding dim** | 384 |
| **Parameters** | ~118M |
| **Speed** | Fast (~500 sentences/sec) |

### T·∫°i sao ch·ªçn model n√†y?

#### ‚úÖ 1. Multilingual (h·ªó tr·ª£ ti·∫øng Vi·ªát)
```python
# Model hi·ªÉu c·∫£ ti·∫øng Vi·ªát v√† ti·∫øng Anh
"K·∫ø To√°n Thu·∫ø"  ‚âà  "Tax Accountant"
# ‚Üí Similar embeddings!
```

Quan tr·ªçng v√¨:
- Job postings ·ªü Vi·ªát Nam th∆∞·ªùng **mix VN-EN**
- V√≠ d·ª•: "IT Project Manager", "K·∫ø To√°n T·ªïng H·ª£p"

#### ‚úÖ 2. Sentence-level (not word-level)
```python
# Word embedding (Word2Vec, GloVe):
"K·∫ø" ‚Üí vector
"To√°n" ‚Üí vector
# Ph·∫£i t·ª± combine

# Sentence embedding (Sentence Transformers):
"K·∫ø To√°n Thu·∫ø" ‚Üí 1 vector duy nh·∫•t
# ƒê√£ capture meaning c·ªßa c·∫£ c√¢u!
```

#### ‚úÖ 3. Pre-trained t·ªët
```python
# Pre-trained on:
# - Paraphrase datasets
# - Translation pairs
# - Q&A pairs
# ‚Üí Learned semantic similarity!
```

#### ‚úÖ 4. Fast & Efficient
```python
# Small model: 118M parameters
# ‚Üí Fast inference
# ‚Üí Kh√¥ng c·∫ßn GPU c≈©ng ch·∫°y ƒë∆∞·ª£c
```

### C√°c alternatives:

| Model | Pros | Cons |
|-------|------|------|
| **PhoBERT** | Specialized for Vietnamese | C·∫ßn fine-tune, larger |
| **mBERT** | Multilingual | Slower, word-level |
| **USE** (Universal Sentence Encoder) | Good quality | English-only |
| **OpenAI Embeddings** | SOTA quality | C·∫ßn API key, cost $ |

‚Üí **paraphrase-multilingual-MiniLM** = best balance!

---

## 4. T√çNH SIMILARITY MATRIX

### M·ª•c ti√™u:

T√≠nh **similarity** (ƒë·ªô t∆∞∆°ng t·ª±) gi·ªØa **m·ªçi c·∫∑p jobs**

### Cosine Similarity:

**Formula:**
$$
\text{similarity}(A, B) = \frac{A \cdot B}{\|A\| \|B\|}
$$

Trong ƒë√≥:
- $A \cdot B$: Dot product
- $\|A\|$: Norm c·ªßa vector A

**Gi√° tr·ªã:**
- 1.0: Ho√†n to√†n gi·ªëng nhau
- 0.0: Kh√¥ng li√™n quan
- -1.0: Ng∆∞·ª£c nghƒ©a (hi·∫øm khi x·∫£y ra v·ªõi embeddings)

**V√≠ d·ª• tr·ª±c quan:**

```
Vector A = [1, 0]
Vector B = [1, 0]  ‚Üí similarity = 1.0 (identical)

Vector A = [1, 0]
Vector B = [0, 1]  ‚Üí similarity = 0.0 (orthogonal)

Vector A = [1, 0]
Vector B = [0.7, 0.7]  ‚Üí similarity = 0.7 (similar direction)
```

### Similarity Matrix:

**ƒê·ªãnh nghƒ©a:** Ma tr·∫≠n 500 √ó 500 ch·ª©a similarity gi·ªØa m·ªçi c·∫∑p jobs

```
           Job1   Job2   Job3   ...   Job500
Job1    [  1.0    0.77   0.23  ...    0.15 ]
Job2    [  0.77   1.0    0.31  ...    0.42 ]
Job3    [  0.23   0.31   1.0   ...    0.68 ]
...
Job500  [  0.15   0.42   0.68  ...    1.0  ]
```

**Properties:**
- **Diagonal = 1.0**: Job so v·ªõi ch√≠nh n√≥ ‚Üí similarity = 1
- **Symmetric**: similarity(A, B) = similarity(B, A)
- **Range**: 0.0 - 1.0

---

## 5. CODE CHI TI·∫æT

### üîπ Class `TextEmbedder`

```python
from sentence_transformers import SentenceTransformer

class TextEmbedder:
    """Generate embeddings for job text data"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or config.EMBEDDING_MODEL
        print(f"Loading embedding model: {self.model_name}")
        self.model = SentenceTransformer(self.model_name)
        print(f"Model loaded! Embedding dimension: {self.model.get_sentence_embedding_dimension()}")
```

**Gi·∫£i th√≠ch:**

```python
self.model = SentenceTransformer(self.model_name)
```
- Load pre-trained model t·ª´ HuggingFace
- L·∫ßn ƒë·∫ßu: Download ~450MB (ch·ªâ 1 l·∫ßn)
- L·∫ßn sau: Load t·ª´ cache (fast)

```python
self.model.get_sentence_embedding_dimension()
```
- L·∫•y s·ªë chi·ªÅu c·ªßa embedding
- ‚Üí 384 cho model n√†y

---

### üîπ Generate Embeddings

```python
def embed_texts(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> np.ndarray:
    """
    Generate embeddings for a list of texts
    
    Args:
        texts: List of text strings (500 jobs)
        batch_size: Batch size for encoding
        show_progress: Whether to show progress bar
        
    Returns:
        numpy array of shape (n_texts, embedding_dim)
        ‚Üí (500, 384)
    """
    print(f"\nEmbedding {len(texts)} texts...")
    embeddings = self.model.encode(
        texts,
        batch_size=batch_size,
        show_progress_bar=show_progress,
        convert_to_numpy=True
    )
    print(f"Generated embeddings with shape: {embeddings.shape}")
    return embeddings
```

**Gi·∫£i th√≠ch t·ª´ng parameter:**

```python
texts = [
    "K·∫ø To√°n Thu·∫ø 3 nƒÉm kinh nghi·ªám...",
    "IT Project Manager...",
    ...
]  # 500 combined_text strings
```

```python
batch_size=32
```
- **Batch processing**: X·ª≠ l√Ω 32 texts c√πng l√∫c thay v√¨ 1
- **T·∫°i sao?** Faster! (GPU/CPU parallelization)
- 500 texts √∑ 32 = ~16 batches

```python
show_progress_bar=True
```
- Hi·ªÉn th·ªã progress bar:
```
Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:11<00:00,  1.41it/s]
```

```python
convert_to_numpy=True
```
- Output format: NumPy array (not Torch tensor)
- Easier to save and manipulate

**Output:**
```python
embeddings.shape = (500, 384)
```
- 500 jobs
- 384 dimensions per job

---

### üîπ Compute Similarity Matrix

```python
def compute_similarity_matrix(self, embeddings: np.ndarray) -> np.ndarray:
    """
    Compute cosine similarity matrix between embeddings
    
    Args:
        embeddings: numpy array of shape (n, dim) ‚Üí (500, 384)
        
    Returns:
        Similarity matrix of shape (n, n) ‚Üí (500, 500)
    """
    print("\nComputing similarity matrix...")
    
    # Step 1: Normalize embeddings
    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
    normalized = embeddings / (norms + 1e-8)
    
    # Step 2: Compute cosine similarity
    similarity = np.dot(normalized, normalized.T)
    
    print(f"Similarity matrix shape: {similarity.shape}")
    print(f"Similarity range: [{similarity.min():.3f}, {similarity.max():.3f}]")
    
    return similarity
```

**Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc:**

#### Step 1: Normalize embeddings

```python
norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
```

**Norm** = ƒë·ªô d√†i c·ªßa vector

V√≠ d·ª•:
```python
vector = [3, 4]
norm = sqrt(3¬≤ + 4¬≤) = sqrt(9 + 16) = 5
```

```python
normalized = embeddings / (norms + 1e-8)
```

**Normalize** = chia cho norm

V√≠ d·ª•:
```python
vector = [3, 4]
norm = 5
normalized = [3/5, 4/5] = [0.6, 0.8]
# Norm of normalized = sqrt(0.6¬≤ + 0.8¬≤) = 1.0 ‚úì
```

**T·∫°i sao normalize?**

Cosine similarity formula:
$$
\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}
$$

N·∫øu ƒë√£ normalize ($\|A\| = \|B\| = 1$):
$$
\cos(\theta) = A \cdot B
$$

‚Üí Ch·ªâ c·∫ßn dot product! (faster computation)

#### Step 2: Compute similarity

```python
similarity = np.dot(normalized, normalized.T)
```

**Matrix multiplication:**

```
normalized: (500, 384)
normalized.T: (384, 500)  ‚Üê Transpose

Result: (500, 500)
```

**M·ªói element:**
```python
similarity[i][j] = dot(normalized[i], normalized[j])
                 = cosine_similarity(job_i, job_j)
```

**Output:**
```python
similarity.shape = (500, 500)
similarity.min() ‚âà -0.091  (√≠t t∆∞∆°ng t·ª±)
similarity.max() = 1.000   (gi·ªëng h·ªát - diagonal)
```

---

### üîπ Find Similar Job Pairs

```python
def find_similar_jobs(
    self, 
    similarity_matrix: np.ndarray, 
    threshold: float = 0.6,
    top_k: int = 10
) -> List[Tuple[int, int, float]]:
    """
    Find similar job pairs based on threshold and top-k
    
    Returns:
        List of (job_i, job_j, similarity) tuples
    """
    n_jobs = similarity_matrix.shape[0]  # 500
    edges = []
    
    for i in range(n_jobs):
        # Get similarities for job i (excluding self)
        sims = similarity_matrix[i].copy()
        sims[i] = -1  # Exclude self-loop
        
        # Get top-K most similar jobs
        top_indices = np.argsort(sims)[-top_k:][::-1]
        
        for j in top_indices:
            if sims[j] >= threshold and i < j:  # Avoid duplicates
                edges.append((i, j, float(sims[j])))
    
    return edges
```

**Gi·∫£i th√≠ch logic:**

#### Loop qua m·ªói job:

```python
for i in range(500):
    sims = similarity_matrix[i]  # L·∫•y row th·ª© i
    # sims = [1.0, 0.77, 0.23, ..., 0.15]
    #         ‚Üë    ‚Üë     ‚Üë           ‚Üë
    #       job_i job_1 job_2     job_499
```

#### Exclude self:

```python
sims[i] = -1
```
- Set similarity v·ªõi ch√≠nh n√≥ = -1
- T·∫°i sao? ƒê·ªÉ kh√¥ng l·∫•y job so v·ªõi ch√≠nh n√≥

#### Find top-K:

```python
top_indices = np.argsort(sims)[-top_k:][::-1]
```

**Gi·∫£i th√≠ch `np.argsort`:**

```python
sims = [0.5, 0.9, 0.3, 0.7]

np.argsort(sims) = [2, 0, 3, 1]  # Indices sorted by value
# sims[2]=0.3 < sims[0]=0.5 < sims[3]=0.7 < sims[1]=0.9

[-top_k:]  # L·∫•y k ph·∫ßn t·ª≠ cu·ªëi (largest)
[::-1]     # Reverse (descending order)
```

**V√≠ d·ª• v·ªõi top_k=3:**
```python
sims = [0.5, 0.9, -1, 0.7, 0.3]  # job[2] = self
argsort = [4, 0, 3, 1, 2]
[-3:] = [3, 1, 2]  # Top 3
[::-1] = [2, 1, 3] = [self, 0.9, 0.7]

# Filter self (similarity[2] = -1 < threshold)
‚Üí Keep [1, 3]  # Jobs with sim 0.9, 0.7
```

#### Filter by threshold v√† avoid duplicates:

```python
if sims[j] >= threshold and i < j:
    edges.append((i, j, float(sims[j])))
```

- `sims[j] >= threshold`: Ch·ªâ l·∫•y similarity ‚â• 0.6
- `i < j`: Tr√°nh duplicate edges
  - Example: (job_1, job_5) v√† (job_5, job_1) l√† gi·ªëng nhau
  - Ch·ªâ l∆∞u (1, 5), kh√¥ng l∆∞u (5, 1)

**Output:**
```python
[
    (0, 23, 0.770),   # Job 0 similar to Job 23 (sim=0.77)
    (0, 40, 0.749),   # Job 0 similar to Job 40 (sim=0.75)
    (1, 15, 0.823),   # Job 1 similar to Job 15 (sim=0.82)
    ...
]
# Total: 2,182 pairs
```

---

## 6. K·∫æT QU·∫¢

### Embeddings:

```python
File: processed/job_embeddings.npy
Shape: (500, 384)
Size: ~768 KB

# M·ªói job ‚Üí 1 vector 384 chi·ªÅu
job_embeddings[0]  # Vector cho Job J001
‚Üí array([0.0234, -0.1567, 0.0891, ..., 0.0245])
```

### Similarity Matrix:

```python
File: processed/similarity_matrix.npy
Shape: (500, 500)
Size: ~1 MB

# Similarity gi·ªØa m·ªçi c·∫∑p jobs
similarity_matrix[0, 23]  # Similarity gi·ªØa Job 0 v√† Job 23
‚Üí 0.770
```

### Similar Job Pairs:

```python
Total: 2,182 pairs
Average similarity: 0.717
Range: 0.600 - 0.999

Top 5 examples:
1. Job 0 ‚Üî Job 23: 0.770
   "K·∫ø To√°n Thu·∫ø" ‚Üî "K·∫ø To√°n T·ªïng H·ª£p"
   
2. Job 0 ‚Üî Job 40: 0.749
   "K·∫ø To√°n Thu·∫ø" ‚Üî "K·∫ø To√°n T·ªïng H·ª£p"
   
3. Job 1 ‚Üî Job 5: 0.823
   "Nh√¢n Vi√™n T√≠n D·ª•ng" ‚Üî "Nh√¢n Vi√™n Thu H·ªìi N·ª£"
   
...
```

### Visualization:

**Similarity Distribution:**

```
[0.6 - 0.65): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 412 pairs
[0.65 - 0.70): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 687 pairs
[0.70 - 0.75): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 589 pairs
[0.75 - 0.80): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 312 pairs
[0.80 - 0.85): ‚ñà‚ñà‚ñà 128 pairs
[0.85 - 0.90): ‚ñà‚ñà 45 pairs
[0.90 - 0.95): ‚ñà 7 pairs
[0.95 - 1.00): ‚ñà 2 pairs
```

‚Üí Most pairs: 0.65 - 0.75 (reasonable similarity)

---

## 7. FAQ

### Q1: T·∫°i sao embedding dimension = 384?
**A:** 
- Trade-off gi·ªØa **quality** v√† **efficiency**
- Nh·ªè h∆°n (128): Faster, nh∆∞ng loss information
- L·ªõn h∆°n (768, 1024): Better quality, nh∆∞ng slower
- 384 = sweet spot cho model n√†y

### Q2: C√≥ th·ªÉ d√πng GPT/OpenAI embeddings kh√¥ng?
**A:** C√≥, nh∆∞ng:
- **Pros**: Quality t·ªët h∆°n (1536 dims)
- **Cons**: 
  - C·∫ßn API key
  - Cost $$ (pay per request)
  - Ph·ª• thu·ªôc internet
  
‚Üí Sentence Transformers = free, offline, good enough!

### Q3: T·∫°i sao threshold = 0.6?
**A:** 
- Empirical choice (th·ª≠ nghi·ªám)
- < 0.6: Qu√° kh√°c nhau, kh√¥ng similar
- ‚â• 0.6: Reasonable similarity
- C√≥ th·ªÉ adjust: 0.5 (nhi·ªÅu edges h∆°n), 0.7 (√≠t edges h∆°n)

### Q4: Top-K = 10 c√≥ ph√π h·ª£p kh√¥ng?
**A:**
- 10 = m·ªói job connect t·ªõi 10 jobs g·∫ßn nh·∫•t
- **Sparse graph**: T·ªët cho GNN (tr√°nh overfitting)
- C√≥ th·ªÉ adjust: 5 (sparser), 20 (denser)

### Q5: Model c√≥ hi·ªÉu ti·∫øng Vi·ªát t·ªët kh√¥ng?
**A:** 
Kh√° t·ªët! V√≠ d·ª•:
```python
"K·∫ø To√°n Thu·∫ø" similar to "K·∫ø To√°n T·ªïng H·ª£p": 0.77 ‚úì
"IT Project Manager" similar to "Project Manager IT": 0.95 ‚úì
```

Nh∆∞ng kh√¥ng perfect:
- Slang, abbreviations c√≥ th·ªÉ kh√¥ng hi·ªÉu
- Industry-specific terms c·∫ßn fine-tuning

### Q6: C√≥ c·∫ßn GPU kh√¥ng?
**A:** **Kh√¥ng b·∫Øt bu·ªôc!**
- CPU: ~11s cho 500 jobs (acceptable)
- GPU: ~2s cho 500 jobs (faster, nh∆∞ng kh√¥ng necessary)

### Q7: Similarity matrix c√≥ sparse kh√¥ng?
**A:** 
```python
# Full matrix: 500 √ó 500 = 250,000 values
# Similarity ‚â• 0.6: 2,182 pairs √∑ 250,000 = 0.87%

‚Üí Very sparse! ‚úì (good for GNN)
```

---

## üìå T√ìM T·∫ÆT

**Input:** 500 combined_text strings

**Process:**
1. ‚úÖ Load Sentence Transformer model (multilingual)
2. ‚úÖ Generate embeddings: 500 √ó 384 vectors
3. ‚úÖ Compute similarity matrix: 500 √ó 500
4. ‚úÖ Find similar pairs: 2,182 edges (threshold ‚â• 0.6)

**Output:** 
- `job_embeddings.npy`: Vector representations
- `similarity_matrix.npy`: Pairwise similarities
- Similar job pairs: For graph construction

**Key insights:**
- Embeddings capture semantic meaning
- Similar jobs have high cosine similarity
- Sparse similarity graph (1.7% density)

---

**üëâ Ti·∫øp theo: [Part 3: Graph Construction](03_Graph_Construction.md)**

---

*Part 2 - Text Embedding | NCKH Project*
